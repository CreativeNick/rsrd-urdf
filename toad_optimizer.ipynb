{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Warp 1.1.0 initialized:\n",
      "   CUDA Toolkit 11.5, Driver 12.2\n",
      "   Devices:\n",
      "     \"cpu\"      : \"x86_64\"\n",
      "     \"cuda:0\"   : \"NVIDIA GeForce RTX 4090\" (24 GiB, sm_89, mempool enabled)\n",
      "   Kernel cache:\n",
      "     /home/chungmin/.cache/warp/1.1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:35:12] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                                                 <a href=\"file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[15:35:12]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m1\u001b[0m                                                 \u001b]8;id=851244;file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=911377;file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\"> Dataset is overriding orientation method to none</span>                                <a href=\"file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#232\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">232</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[33m Dataset is overriding orientation method to none\u001b[0m                                \u001b]8;id=418932;file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=512413;file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#232\u001b\\\u001b[2m232\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\"> Dataset is overriding orientation method to none</span>                                <a href=\"file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#232\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">232</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[33m Dataset is overriding orientation method to none\u001b[0m                                \u001b]8;id=518131;file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=723602;file:///home/chungmin/Documents/please2/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#232\u001b\\\u001b[2m232\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m57\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4b97a84bc346218c66543f8c66826d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "outputs/buddha_balls_poly/garfield/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">05</span>-23_152531/nerfstudio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000008000.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "outputs/buddha_balls_poly/garfield/\u001b[1;36m2024\u001b[0m-\u001b[1;36m05\u001b[0m-23_152531/nerfstudio_models/step-\u001b[1;36m000008000.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'GarfieldDataManager' object has no attribute 'dino_dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m OUTPUT_FOLDER\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m train_config,pipeline,_,_ \u001b[38;5;241m=\u001b[39m eval_setup(config)\n\u001b[0;32m---> 56\u001b[0m dino_loader \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatamanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdino_dataloader\u001b[49m\n\u001b[1;32m     57\u001b[0m train_config\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mlocal_writer\u001b[38;5;241m.\u001b[39menable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# We need to set up the writer to track number of rays, otherwise the viewer will not calculate the resolution correctly\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/please2/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GarfieldDataManager' object has no attribute 'dino_dataloader'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell loads the model from the config file and initializes the viewer\n",
    "'''\n",
    "# %matplotlib widget\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nerfstudio.viewer.viewer import Viewer\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from nerfstudio.utils import writer\n",
    "import time\n",
    "from threading import Lock\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import resize\n",
    "from lerf.zed import Zed\n",
    "import warp as wp\n",
    "from toad.optimization.rigid_group_optimizer import RigidGroupOptimizer\n",
    "from toad.optimization.atap_loss import ATAPLoss\n",
    "from toad.utils import *\n",
    "wp.init()\n",
    "\n",
    "# config = Path(\"outputs/nerfgun2/dig/2024-05-03_161203/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun3/dig/2024-05-03_170424/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun4/dig/2024-05-07_130351/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-10_132522/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-16_233028/config.yml\")#with ruilongs v2\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-09_123412/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-16_231213/config.yml\")#with ruilongs v2\n",
    "# config = Path(\"outputs/cal_bear/dig/2024-05-15_155531/config.yml\")#this one groups table with bear for some reason\n",
    "# config = Path(\"outputs/bww_faucet/dig/2024-05-12_215440/config.yml\")\n",
    "# config = Path(\"outputs/cmk_tpose2/dig/2024-05-14_142439/config.yml\")\n",
    "# config = Path(\"outputs/cal_bear/dig/2024-05-17_142920/config.yml\")#ruilong v2\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-17_145312/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger2/dig/2024-05-17_152545/config.yml\")\n",
    "# config = Path(\"outputs/glue_gun/dig/2024-05-17_161408/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-19_122050/config.yml\")# reuilong v2, 32-dim gauss\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-19_125443/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger2/dig/2024-05-19_132100/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-20_191616/config.yml\")#with antialias\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-20_192646/config.yml\")\n",
    "# config = Path(\"outputs/garfield_plushie/dig/2024-05-21_144709/config.yml\")\n",
    "# config=Path(\"outputs/boops_poly/dig/2024-05-22_000924/config.yml\")\n",
    "# config=Path(\"outputs/buddha_balls/dig/2024-05-23_145359/config.yml\")\n",
    "config=Path(\"outputs/buddha_balls_poly/dig/2024-05-23_153552/config.yml\")\n",
    "OUTPUT_FOLDER = Path(\"renders/buddha_balls_poly\")\n",
    "\n",
    "assert OUTPUT_FOLDER.stem in str(config), \"Output folder name does not match config name\"\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True,parents=True)\n",
    "train_config,pipeline,_,_ = eval_setup(config)\n",
    "dino_loader = pipeline.datamanager.dino_dataloader\n",
    "train_config.logging.local_writer.enable = False\n",
    "# We need to set up the writer to track number of rays, otherwise the viewer will not calculate the resolution correctly\n",
    "writer.setup_local_writer(train_config.logging, max_iter=train_config.max_num_iterations)\n",
    "v = Viewer(ViewerConfig(default_composite_depth=False,num_rays_per_chunk=-1),config.parent,pipeline.datamanager.get_datapath(),pipeline,train_lock=Lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell defines a simple pose optimizer for learning a rigid transform offset given a gaussian model, star pose, and starting view\n",
    "\"\"\"\n",
    "\n",
    "def get_vid_frame(cap,timestamp):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame number based on the timestamp and fps\n",
    "    frame_number = min(int(timestamp * fps),int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1))\n",
    "    \n",
    "    # Set the video position to the calculated frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    success, frame = cap.read()\n",
    "    # convert BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n",
    "\n",
    "MATCH_RESOLUTION = 500\n",
    "camera_input = 'zed' # ['iphone','zed', 'iphone_vertical','zed_svo']\n",
    "video_path = Path(\"motion_vids/boops_lift.MOV\")\n",
    "svo_path = Path(\"motion_vids/buddha_remove_good.svo2\")\n",
    "start_time = 0.3\n",
    "import viser.transforms as vtf\n",
    "cam_pose = None #pipeline.viewer_control.get_camera(200,None,0)\n",
    "if cam_pose is None:\n",
    "    R = vtf.SO3.from_y_radians(-np.pi/2).as_matrix()\n",
    "    H = np.eye(4)\n",
    "    H[:3,:3] = R\n",
    "    cam_pose = torch.from_numpy(H).float()[None,:3,:]#TODO ground truth cam to yumi\n",
    "if camera_input == 'iphone':\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fx = 1137.0,fy = 1137.0,cx = 1280.0/2,cy = 720/2,width=1280,height=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone_vertical':\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fy = 1137.0,fx = 1137.0,cy = 1280/2,cx = 720/2,height=1280,width=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input in ['zed','zed_svo']:\n",
    "    try:\n",
    "        zed.cam.close()\n",
    "        del zed\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        zed = Zed(recording_file=str(svo_path.absolute()) if camera_input == 'zed_svo' else None, start_time=start_time)\n",
    "    fps = 30    \n",
    "    left_rgb,_,_ = zed.get_frame()\n",
    "    K = zed.get_K()\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fx = K[0,0],fy = K[1,1],cx = K[0,2],cy = K[1,2],width=1920,height=1080)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "outputs = pipeline.model.get_outputs_for_camera(init_cam)\n",
    "if pipeline.cluster_labels is not None:\n",
    "    labels = pipeline.cluster_labels.int().cuda()\n",
    "    group_masks = [(cid == labels).cuda() for cid in range(labels.max() + 1)]\n",
    "else:\n",
    "    labels = torch.zeros(pipeline.model.num_points).int().cuda()\n",
    "    group_masks = [torch.ones(pipeline.model.num_points).bool().cuda()]\n",
    "optimizer = RigidGroupOptimizer(pipeline.model,dino_loader,init_cam,group_masks, group_labels = labels, render_lock = v.train_lock)\n",
    "rgb_renders = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if camera_input in ['zed','zed_svo']:\n",
    "    left_rgb, right_rgb,depth = zed.get_frame()\n",
    "    target_frame_rgb = (left_rgb/255)\n",
    "    right_frame_rgb = (right_rgb/255)\n",
    "    optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "else:\n",
    "    assert video_path.exists()\n",
    "    motion_clip = cv2.VideoCapture(str(video_path.absolute()))\n",
    "    start=1\n",
    "    end=9\n",
    "    fps = 30\n",
    "    frame = get_vid_frame(motion_clip,start)\n",
    "    target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "    optimizer.set_frame(target_frame_rgb)\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "xs,ys,outputs,best_pose,renders,best_pix = optimizer.initialize_obj_pose()\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "rescale = max(target_frame_rgb.shape[0],target_frame_rgb.shape[1])/MATCH_RESOLUTION\n",
    "best_pix = best_pix*rescale\n",
    "axs[1].scatter(xs.cpu().numpy()*rescale,ys.cpu().numpy()*rescale)\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())\n",
    "axs[0].imshow(optimizer.rgb_frame.cpu().numpy(),alpha=.3)\n",
    "renders = [r.detach().cpu().numpy()*255 for r in renders]\n",
    "#save video as test_camopt.mp4\n",
    "out_clip = mpy.ImageSequenceClip(renders, fps=30)  \n",
    "out_clip.write_videofile(\"test_camopt.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pose\n",
    "init_cam.camera_to_worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "import tqdm\n",
    "import moviepy.editor as mpy\n",
    "import plotly.express as px\n",
    "def plotly_render(frame):\n",
    "    fig = px.imshow(frame)\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),showlegend=False,yaxis_visible=False, yaxis_showticklabels=False,xaxis_visible=False, xaxis_showticklabels=False\n",
    "    )\n",
    "    return fig\n",
    "fig = plotly_render(outputs['rgb'].detach().cpu().numpy())\n",
    "try:\n",
    "    frame_vis.remove()\n",
    "except:\n",
    "    pass\n",
    "frame_vis = pipeline.viewer_control.viser_server.add_gui_plotly(fig, 9/16)\n",
    "try:\n",
    "    animate_button.remove()\n",
    "    frame_slider.remove()\n",
    "    reset_button.remove()\n",
    "except:\n",
    "    pass\n",
    "def composite_vis_frame(target_frame_rgb,outputs):\n",
    "    target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "    # composite the outputs['rgb'] on top of target_vis frame\n",
    "    target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "    return target_vis_frame\n",
    "\n",
    "try:\n",
    "    render_button.remove()\n",
    "    filename_input.remove()\n",
    "    status_mkdown.remove()\n",
    "except:\n",
    "    pass\n",
    "import viser\n",
    "filename_input = v.viser_server.add_gui_text(\"File Name\",\"render\")\n",
    "status_mkdown = v.viser_server.add_gui_markdown(\" \")\n",
    "render_button = v.viser_server.add_gui_button(\"Render Animation\",color='green',icon=viser.Icon.MOVIE)\n",
    "@render_button.on_click\n",
    "def render(_):\n",
    "    render_button.disabled = True\n",
    "    render_frames = []\n",
    "    camera = pipeline.viewer_control.get_camera(1080,1920,0)\n",
    "    for i in tqdm.tqdm(range(len(optimizer.keyframes))):\n",
    "        status_mkdown.content = f\"Rendering...{i/len(optimizer.keyframes):.01f}\"\n",
    "        pipeline.model.eval()\n",
    "        optimizer.apply_keyframe(i)\n",
    "        with torch.no_grad():\n",
    "            outputs = pipeline.model.get_outputs_for_camera(camera)\n",
    "        render_frames.append(outputs[\"rgb\"].detach().cpu().numpy()*255)\n",
    "    status_mkdown.content = \"Saving...\"\n",
    "    out_clip = mpy.ImageSequenceClip(render_frames, fps=fps)\n",
    "    fname = filename_input.value\n",
    "    (OUTPUT_FOLDER / 'posed_renders').mkdir(exist_ok=True)\n",
    "    render_folder = OUTPUT_FOLDER / 'posed_renders'\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}.mp4\", fps=fps,codec='libx264')\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}_mac.mp4\", fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "    v.viser_server.send_file_download(f\"{fname}_mac.mp4\",open(f\"{render_folder}/{fname}_mac.mp4\",'rb').read())\n",
    "    status_mkdown.content = \"Done!\"\n",
    "    render_button.disabled = False\n",
    "\n",
    "\n",
    "if camera_input in ['zed','zed_svo']:\n",
    "    if len(rgb_renders)==0:\n",
    "        for i in tqdm.tqdm(range(10)):\n",
    "            target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "            vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "            fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "            frame_vis.figure = fig\n",
    "            rgb_renders.append(vis_frame*255)\n",
    "            outputs = optimizer.step(50, use_depth=i>7, metric_depth=True)\n",
    "    while True:\n",
    "        # If input camera is the zed, just loop it indefinitely until no more frames\n",
    "        left_rgb, _, depth = zed.get_frame()\n",
    "        if left_rgb is None:\n",
    "            break\n",
    "        target_frame_rgb = left_rgb/255\n",
    "        optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "        outputs = optimizer.step(50, metric_depth=True)\n",
    "        v._trigger_rerender()\n",
    "        optimizer.register_keyframe()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "elif camera_input in ['iphone','iphone_vertical','train_cam']:\n",
    "    # Otherwise procces the video\n",
    "    if len(rgb_renders)==0:\n",
    "        for i in tqdm.tqdm(range(10)):\n",
    "            target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "            vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "            fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "            frame_vis.figure = fig\n",
    "            rgb_renders.append(vis_frame*255)\n",
    "            outputs = optimizer.step(30, use_depth=i>7, metric_depth=False)\n",
    "\n",
    "    for t in tqdm.tqdm(np.linspace(start,end,int((end-start)*fps))):\n",
    "        frame = get_vid_frame(motion_clip,t)\n",
    "        target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "        optimizer.set_frame(target_frame_rgb)\n",
    "        outputs = optimizer.step(50, metric_depth=False)\n",
    "        optimizer.register_keyframe()\n",
    "        v._trigger_rerender()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "#save as an mp4\n",
    "out_clip = mpy.ImageSequenceClip(rgb_renders, fps=fps)  \n",
    "\n",
    "fname = str(OUTPUT_FOLDER / \"optimizer_out_antialias_withdepth_highatap.mp4\")\n",
    "\n",
    "out_clip.write_videofile(fname, fps=fps,codec='libx264')\n",
    "out_clip.write_videofile(fname.replace('.mp4','_mac.mp4'),fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "\n",
    "# Populate some viewer elements to visualize the animation\n",
    "animate_button = v.viser_server.add_gui_button(\"Play Animation\")\n",
    "frame_slider = v.viser_server.add_gui_slider(\"Frame\",0,len(optimizer.keyframes)-1,1,0)\n",
    "reset_button = v.viser_server.add_gui_button(\"Reset Transforms\")\n",
    "\n",
    "@animate_button.on_click\n",
    "def play_animation(_):\n",
    "    for i in range(len(optimizer.keyframes)):\n",
    "        optimizer.apply_keyframe(i)\n",
    "        v._trigger_rerender()\n",
    "        time.sleep(1/fps)\n",
    "@frame_slider.on_update\n",
    "def apply_keyframe(_):\n",
    "    optimizer.apply_keyframe(frame_slider.value)\n",
    "    v._trigger_rerender()\n",
    "@reset_button.on_click\n",
    "def reset_transforms(_):\n",
    "    optimizer.reset_transforms()\n",
    "    v._trigger_rerender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
