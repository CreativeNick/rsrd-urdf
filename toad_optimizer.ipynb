{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "'''\n",
    "This cell loads the model from the config file and initializes the viewer\n",
    "'''\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nerfstudio.viewer.viewer import Viewer\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from nerfstudio.utils import writer\n",
    "import time\n",
    "from threading import Lock\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import resize\n",
    "from toad.zed import Zed\n",
    "import warp as wp\n",
    "from toad.optimization.rigid_group_optimizer import RigidGroupOptimizer\n",
    "from toad.optimization.atap_loss import ATAPLoss\n",
    "from toad.utils import *\n",
    "from toad.hand_registration import HandRegistration\n",
    "from toad.optimization.observation import PosedObservation\n",
    "wp.init()\n",
    "\n",
    "# config = Path(\"outputs/buddha_empty/dig/2024-06-02_224243/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun_poly_far/dig/2024-06-02_234451/config.yml\")\n",
    "# config = Path(\"outputs/scissors/dig/2024-06-03_135548/config.yml\")#pretty decent\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-06-03_101237/config.yml\")\n",
    "# config = Path(\"outputs/wooden_drawer/dig/2024-06-03_160055/config.yml\")#works on slide, wiggle is a bit shaky but not bad\n",
    "# config = Path(\"outputs/sunglasses/dig/2024-06-03_161521/config.yml\")#works, but some noise so rescanning\n",
    "# config = Path(\"outputs/sunglasses3/dig/2024-06-03_175202/config.yml\")# scan is much better, works well\n",
    "# config = Path(\"outputs/charger_poly/dig/2024-06-03_164048/config.yml\")#doesn't work, scan very noisy, rescanning\n",
    "# config = Path(\"outputs/charger_poly3/dig/2024-06-03_173148/config.yml\")\n",
    "# config = Path(\"outputs/garfield_poly/dig/2024-06-03_183227/config.yml\")#IS A BIT translucent, maybe bad?\n",
    "# config = Path(\"outputs/charger_latch/dig/2024-06-05_101644/config.yml\")#doesn't work\n",
    "# config = Path(\"outputs/painter_t/dig/2024-06-05_103134/config.yml\")#somewhat works, might be usable\n",
    "# config = Path(\"outputs/sanitizer/dig/2024-06-03_ 222921/config.yml\")\n",
    "# config = Path(\"outputs/big_painter_t/dig/2024-06-05_122620/config.yml\")\n",
    "# config = Path(\"outputs/power_brick/dig/2024-06-05_125218/config.yml\")#bad recon.....trying again\n",
    "# config = Path(\"outputs/power_brick3_colmap/dig/2024-06-05_150027/config.yml\")\n",
    "# config = Path(\"outputs/panini/dig/2024-06-05_193512/config.yml\")\n",
    "# config = Path(\"outputs/cal_bear_naked/dig/2024-06-04_215034/config.yml\")\n",
    "\n",
    "\n",
    "# config = Path(\"outputs/buddha_empty/dig/2024-06-09_173902/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-06-09_003734/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-07-02_202955/config.yml\")#48 dim\n",
    "# config = Path(\"outputs/cal_bear_naked/dig/2024-06-10_131301/config.yml\")\n",
    "# config = Path(\"outputs/sunglasses3/dig/2024-07-02_070651/config.yml\")\n",
    "# config = Path(\"outputs/garfield_poly/dig/2024-06-09_005651/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun_poly_far/dig/2024-06-09_010536/config.yml\")\n",
    "# config = Path(\"outputs/scissors/dig/2024-06-10_133700/config.yml\")\n",
    "\n",
    "# config = Path(\"outputs/big_painter_t/dig/2024-06-09_011522/config.yml\")\n",
    "# config = Path(\"outputs/big_painter_t/dig/2024-07-03_014541/config.yml\")#no denoise or registers\n",
    "# config = Path(\"outputs/wooden_drawer/dig/2024-06-10_134919/config.yml\")\n",
    "# config = Path(\"outputs/scissors/dig/2024-07-03_015411/config.yml\")#no denoise, use .9 for state frame\n",
    "\n",
    "# config = Path(\"outputs/nerfgun2/dig/2024-06-11_142428/config.yml\")\n",
    "# config = Path(\"outputs/purple_flower/dig/2024-06-30_233334/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun_poly_far/dig/2024-07-03_000524/config.yml\")#without registers, no denoising\n",
    "\n",
    "# config = Path(\"outputs/buddha_empty/dig/2024-07-03_191729/config.yml\")#vit-l\n",
    "\n",
    "# config = Path(\"outputs/buddha_empty/dig/2024-07-03_195352/config.yml\")#vit-l with 64 dim\n",
    "# config = Path(\"outputs/purple_flower/dig/2024-07-03_200636/config.yml\")#vit-l with 64 dim\n",
    "# config = Path(\"outputs/nerfgun_poly_far/dig/2024-07-03_211551/config.yml\")#vit-l with 64 dim\n",
    "config = Path(\"outputs/sunglasses3/dig/2024-07-03_225001/config.yml\")#vit-l with 64 dim\n",
    "OUTPUT_FOLDER = Path(\"renders/refactor/sunglasses/\")\n",
    "\n",
    "HANDS = False\n",
    "\n",
    "assert OUTPUT_FOLDER.stem in str(config), \"Output folder name does not match config name\"\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True,parents=True)\n",
    "train_config,pipeline,_,_ = eval_setup(config)\n",
    "\n",
    "dino_loader = pipeline.datamanager.dino_dataloader\n",
    "train_config.logging.local_writer.enable = False\n",
    "# We need to set up the writer to track number of rays, otherwise the viewer will not calculate the resolution correctly\n",
    "writer.setup_local_writer(train_config.logging, max_iter=train_config.max_num_iterations)\n",
    "v = Viewer(ViewerConfig(default_composite_depth=False,num_rays_per_chunk=-1),config.parent,pipeline.datamanager.get_datapath(),pipeline,train_lock=Lock())\n",
    "try:\n",
    "    pipeline.load_state()\n",
    "except FileNotFoundError:\n",
    "    print(\"No state found, starting from scratch\")\n",
    "if HANDS:\n",
    "    handreg = HandRegistration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell defines a simple pose optimizer for learning a rigid transform offset given a gaussian model, star pose, and starting view\n",
    "\"\"\"\n",
    "\n",
    "def get_vid_frame(cap,timestamp):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame number based on the timestamp and fps\n",
    "    frame_number = min(int(timestamp * fps),int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1))\n",
    "    \n",
    "    # Set the video position to the calculated frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    success, frame = cap.read()\n",
    "    # convert BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n",
    "\n",
    "MATCH_RESOLUTION = 500\n",
    "camera_input = 'iphone' # ['iphone','zed', 'iphone_vertical','zed_svo']\n",
    "video_path = Path(\"motion_vids/sunglasses_fold.MOV\")\n",
    "svo_path = Path(\"motion_vids/sunglasses2.svo2\")\n",
    "start_time = .3\n",
    "\n",
    "cam_pose = None\n",
    "if cam_pose is None:\n",
    "    H = np.eye(4)\n",
    "    from viser import transforms as vtf\n",
    "    H[:3,:3] = vtf.SO3.from_x_radians(np.pi/4).as_matrix()\n",
    "    cam_pose = torch.from_numpy(H).float()[None,:3,:]#TODO ground truth cam to yumi\n",
    "if camera_input == 'iphone':\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fx = 1137.0,fy = 1137.0,cx = 1280.0/2,cy = 720/2,width=1280,height=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone_vertical':\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fy = 1137.0,fx = 1137.0,cy = 1280/2,cx = 720/2,height=1280,width=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input in ['zed','zed_svo']:\n",
    "    try:\n",
    "        zed.cam.close()\n",
    "        del zed\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        zed = Zed(recording_file=str(svo_path.absolute()) if camera_input == 'zed_svo' else None, start_time=start_time)\n",
    "    fps = 30    \n",
    "    left_rgb,_,_ = zed.get_frame(depth=False)\n",
    "    K = zed.get_K()\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fx = K[0,0],fy = K[1,1],cx = K[0,2],cy = K[1,2],width=left_rgb.shape[1],height=left_rgb.shape[0])\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "pipeline.model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = pipeline.model.get_outputs_for_camera(init_cam)\n",
    "if pipeline.cluster_labels is not None:\n",
    "    labels = pipeline.cluster_labels.int().cuda()\n",
    "    group_masks = [(cid == labels).cuda() for cid in range(labels.max() + 1)]\n",
    "else:\n",
    "    labels = torch.zeros(pipeline.model.num_points).int().cuda()\n",
    "    group_masks = [torch.ones(pipeline.model.num_points).bool().cuda()]\n",
    "optimizer = RigidGroupOptimizer(pipeline.model,dino_loader,group_masks, group_labels = labels, dataset_scale = pipeline.datamanager.train_dataset._dataparser_outputs.dataparser_scale, render_lock = v.train_lock)\n",
    "rgb_renders = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_fn = lambda x: dino_loader.get_pca_feats(x, keep_cuda=True)\n",
    "if camera_input in ['zed','zed_svo']:\n",
    "    left_rgb, right_rgb,depth = zed.get_frame()\n",
    "    target_frame_rgb = (left_rgb/255)\n",
    "    right_frame_rgb = (right_rgb/255)\n",
    "    frame = PosedObservation(target_frame_rgb, init_cam, dino_fn, metric_depth_img=depth)\n",
    "else:\n",
    "    assert video_path.exists()\n",
    "    motion_clip = cv2.VideoCapture(str(video_path.absolute()))\n",
    "    start=1\n",
    "    end=3\n",
    "    fps = 30\n",
    "    frame = get_vid_frame(motion_clip,start)\n",
    "    target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "    frame = PosedObservation(target_frame_rgb, init_cam, dino_fn)\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "xs,ys,outputs,renders = optimizer.initialize_obj_pose(frame, render=False, niter=200, n_seeds=6)\n",
    "_,axs = plt.subplots(1,2,figsize=(11,5))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "rescale = max(target_frame_rgb.shape[0],target_frame_rgb.shape[1])/MATCH_RESOLUTION\n",
    "axs[1].scatter(xs.cpu().numpy()*rescale,ys.cpu().numpy()*rescale)\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())\n",
    "axs[0].imshow(frame.rgb.cpu().numpy(),alpha=.35)\n",
    "if len(renders)>1:\n",
    "    renders = [r.detach().cpu().numpy()*255 for r in renders]\n",
    "    # save video as test_camopt.mp4\n",
    "    out_clip = mpy.ImageSequenceClip(renders, fps=30)\n",
    "    out_clip.write_videofile(\"test_camopt.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import moviepy.editor as mpy\n",
    "import plotly.express as px\n",
    "import viser\n",
    "import trimesh\n",
    "\n",
    "def plotly_render(frame):\n",
    "    fig = px.imshow(frame)\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),showlegend=False,yaxis_visible=False, yaxis_showticklabels=False,xaxis_visible=False, xaxis_showticklabels=False\n",
    "    )\n",
    "    return fig\n",
    "fig = plotly_render(outputs['rgb'].detach().cpu().numpy())\n",
    "try:\n",
    "    frame_vis.remove()\n",
    "except:\n",
    "    pass\n",
    "frame_vis = pipeline.viewer_control.viser_server.add_gui_plotly(fig, 9/16)\n",
    "try:\n",
    "    animate_button.remove()\n",
    "    frame_slider.remove()\n",
    "    reset_button.remove()\n",
    "except:\n",
    "    pass\n",
    "def composite_vis_frame(target_frame_rgb,outputs):\n",
    "    target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1]),antialias=True).permute(1,2,0)\n",
    "    # composite the outputs['rgb'] on top of target_vis frame\n",
    "    target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"].detach()*0.5\n",
    "    return target_vis_frame\n",
    "\n",
    "\n",
    "from typing import Tuple,Optional,List\n",
    "\n",
    "def get_hands(frame, framedepth, optimizer, outputs) -> Tuple[Optional[List[trimesh.Trimesh]],Optional[List[trimesh.Trimesh]]]:\n",
    "    left_hand,right_hand = handreg.detect_hands(frame,optimizer.init_c2w.fx.item()*(max(frame.shape[0],frame.shape[1])/MATCH_RESOLUTION))\n",
    "    for i,hands in enumerate([left_hand,right_hand]):\n",
    "        if hands is None:continue\n",
    "        hands['trimeshes'] = []\n",
    "        #Compute hand shift to align with gaussians\n",
    "        #resize framedepth to the same size as the rendered frame\n",
    "        framedepth = resize(\n",
    "                    framedepth.permute(2, 0, 1),\n",
    "                    (outputs['rgb'].shape[0], outputs['rgb'].shape[1]),\n",
    "                    antialias = True,\n",
    "                ).permute(1, 2, 0)\n",
    "        handreg.align_hands(hands,outputs['depth'].detach()/optimizer.dataset_scale, framedepth, outputs['accumulation'].detach()>.8,optimizer.init_c2w.fx.item())\n",
    "        # visualize result\n",
    "        for j in range(hands['verts'].shape[0]):\n",
    "            vertices = hands['verts'][j]\n",
    "            faces = hands['faces']\n",
    "            mesh = trimesh.Trimesh(vertices, faces)\n",
    "            cam_pose = torch.eye(4)\n",
    "            cam_pose[:3,:] = optimizer.init_c2w.camera_to_worlds\n",
    "            cam_pose[1:3,:] *= -1\n",
    "            mesh.apply_transform(cam_pose.cpu().numpy())\n",
    "\n",
    "            mesh.vertices = mesh.vertices*optimizer.dataset_scale\n",
    "            v.viser_server.add_mesh_trimesh(f\"hand{i}_{j}\",mesh,scale=10)\n",
    "            hands['trimeshes'].append(mesh)\n",
    "    return [] if left_hand is None else left_hand['trimeshes'],[] if right_hand is None else right_hand['trimeshes']\n",
    "\n",
    "\n",
    "if camera_input in ['zed','zed_svo']:\n",
    "    if HANDS:\n",
    "        lh,rh = get_hands(target_frame_rgb.cpu().numpy(),depth,optimizer,outputs)\n",
    "        optimizer.register_keyframe(lhands = lh, rhands = rh)\n",
    "    while True:\n",
    "        # If input camera is the zed, just loop it indefinitely until no more frames\n",
    "        left_rgb, _, depth = zed.get_frame()\n",
    "        if left_rgb is None:\n",
    "            break\n",
    "        target_frame_rgb = left_rgb/255\n",
    "        optim_frame = PosedObservation(target_frame_rgb, init_cam, dino_fn)\n",
    "        optimizer.add_frame(optim_frame)\n",
    "        outputs = optimizer.step(25)\n",
    "        v._trigger_rerender()\n",
    "        print(\"Frame\",len(rgb_renders))\n",
    "        if HANDS:\n",
    "            lhands,rhands = get_hands((target_frame_rgb*255).to(torch.uint8).cpu().numpy(),depth[...,None],optimizer,outputs)\n",
    "        else:\n",
    "            lhands,rhands = [],[]\n",
    "        optimizer.register_keyframe(lhands = lhands, rhands = rhands)\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "elif camera_input in ['iphone','iphone_vertical','train_cam']:\n",
    "    # Otherwise procces the video\n",
    "    if HANDS:\n",
    "        depth = get_depth((target_frame_rgb*255).to(torch.uint8))\n",
    "        lh,rh = get_hands(target_frame_rgb.cpu().numpy(),1/depth,optimizer,outputs)\n",
    "        optimizer.register_keyframe(lhands = lh, rhands = rh)\n",
    "    for t in tqdm.tqdm(np.linspace(start,end,int((end-start)*fps))):\n",
    "        frame = get_vid_frame(motion_clip,t)\n",
    "        target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "        optim_frame = PosedObservation(target_frame_rgb, init_cam, dino_fn)\n",
    "        optimizer.add_frame(optim_frame)\n",
    "        outputs = optimizer.step(25)\n",
    "        if HANDS:\n",
    "            lhands,rhands = get_hands(frame,1/get_depth((target_frame_rgb*255).to(torch.uint8))[...,None],optimizer,outputs)\n",
    "        else:\n",
    "            lhands,rhands = [],[]\n",
    "        optimizer.register_keyframe(lhands = lhands, rhands = rhands)\n",
    "        v._trigger_rerender()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"].detach(),target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "#save as an mp4\n",
    "out_clip = mpy.ImageSequenceClip(rgb_renders, fps=fps)  \n",
    "\n",
    "#save rendering video\n",
    "fname = str(OUTPUT_FOLDER / \"optimizer_out.mp4\")\n",
    "out_clip.write_videofile(fname, fps=fps,codec='libx264')\n",
    "out_clip.write_videofile(fname.replace('.mp4','_mac.mp4'),fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "\n",
    "#save part trajectories\n",
    "optimizer.save_trajectory(OUTPUT_FOLDER / \"keyframes.pt\")\n",
    "print(\"Saved keyframes to\",OUTPUT_FOLDER / \"keyframes.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = optimizer.step(50,all_frames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import viser\n",
    "import tqdm\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "# optimizer.load_trajectory(OUTPUT_FOLDER / \"keyframes.pt\")\n",
    "# optimizer.is_initialized = True\n",
    "# Populate some viewer elements to visualize the animation\n",
    "try:\n",
    "    render_button.remove()\n",
    "    filename_input.remove()\n",
    "    status_mkdown.remove()\n",
    "    render_video_view.remove()\n",
    "except:\n",
    "    pass\n",
    "animate_button = v.viser_server.gui.add_button(\"Play Animation\")\n",
    "frame_slider = v.viser_server.gui.add_slider(\"Frame\",0,len(optimizer.sequence)-1,1,0)\n",
    "reset_button = v.viser_server.gui.add_button(\"Reset Transforms\")\n",
    "fps = 30\n",
    "filename_input = v.viser_server.gui.add_text(\"File Name\",\"render\")\n",
    "render_video_view = v.viser_server.gui.add_checkbox(\"From Video View\",False)\n",
    "status_mkdown = v.viser_server.gui.add_markdown(\" \")\n",
    "render_button = v.viser_server.gui.add_button(\"Render Animation\",color='green',icon=viser.Icon.MOVIE)\n",
    "@render_button.on_click\n",
    "def render(_):\n",
    "    render_button.disabled = True\n",
    "    render_frames = []\n",
    "    camera = pipeline.viewer_control.get_camera(1080,1920,0)\n",
    "    for i in tqdm.tqdm(range(len(optimizer.sequence))):\n",
    "        status_mkdown.content = f\"Rendering...{i/len(optimizer.sequence):.01f}\"\n",
    "        pipeline.model.eval()\n",
    "        optimizer.apply_keyframe(i)\n",
    "        box = v.control_panel.crop_obb\n",
    "        with torch.no_grad():\n",
    "            if render_video_view.value:\n",
    "                frame = optimizer.sequence.get_frame(i)\n",
    "                frame_outputs = pipeline.model.get_outputs_for_camera(frame.camera)\n",
    "                target_vis_frame = composite_vis_frame(frame.rgb,frame_outputs)\n",
    "                vis_frame = torch.cat([frame_outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()*255\n",
    "                render_frames.append(vis_frame)\n",
    "            else:\n",
    "                outputs = pipeline.model.get_outputs_for_camera(camera)\n",
    "                render_frames.append(outputs[\"rgb\"].detach().cpu().numpy()*255)\n",
    "    status_mkdown.content = \"Saving...\"\n",
    "    out_clip = mpy.ImageSequenceClip(render_frames, fps=fps)\n",
    "    fname = filename_input.value\n",
    "    (OUTPUT_FOLDER / 'posed_renders').mkdir(exist_ok=True)\n",
    "    render_folder = OUTPUT_FOLDER / 'posed_renders'\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}.mp4\", fps=fps,codec='libx264')\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}_mac.mp4\", fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "    v.viser_server.send_file_download(f\"{fname}_mac.mp4\",open(f\"{render_folder}/{fname}_mac.mp4\",'rb').read())\n",
    "    status_mkdown.content = \"Done!\"\n",
    "    render_button.disabled = False\n",
    "@animate_button.on_click\n",
    "def play_animation(_):\n",
    "    for i in range(len(optimizer.sequence)):\n",
    "        optimizer.apply_keyframe(i)\n",
    "        hands = optimizer.hand_frames[i]\n",
    "        for ih,h in enumerate(hands):\n",
    "            h_world = h.copy()\n",
    "            h_world.apply_transform(optimizer.get_registered_o2w().cpu().numpy())\n",
    "            v.viser_server.add_mesh_trimesh(f\"hand{ih}\",h_world,scale=10)\n",
    "        v._trigger_rerender()\n",
    "        time.sleep(1/fps)\n",
    "@frame_slider.on_update\n",
    "def apply_keyframe(_):\n",
    "    optimizer.apply_keyframe(frame_slider.value)\n",
    "    hands = optimizer.hand_frames[frame_slider.value]\n",
    "    for ih,h in enumerate(hands):\n",
    "        h_world = h.copy()\n",
    "        h_world.apply_transform(optimizer.get_registered_o2w().cpu().numpy())\n",
    "        v.viser_server.add_mesh_trimesh(f\"hand{ih}\", h_world, scale=10)\n",
    "    v._trigger_rerender()\n",
    "@reset_button.on_click\n",
    "def reset_transforms(_):\n",
    "    optimizer.reset_transforms()\n",
    "    v._trigger_rerender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAN IGNORE EVERYTHING FROM HERE, MOSTLY FIGURE MAKING CODE\n",
    "pipeline.reset_colors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.colormaps import apply_pca_colormap\n",
    "for g in range(len(optimizer.group_masks)):\n",
    "    centroid = optimizer.dig_model.means[optimizer.group_masks[g]].mean(0).detach().cpu().numpy()\n",
    "    v.viser_server.scene.add_frame(f\"/centeers/group{g}_center\",position=centroid*10)\n",
    "camera = pipeline.viewer_control.get_camera(281,500,0)\n",
    "with torch.no_grad():\n",
    "    pipeline.model.eval()\n",
    "    cam_outputs = pipeline.model.get_outputs_for_camera(camera)\n",
    "\n",
    "\n",
    "#video frame\n",
    "motion_clip = cv2.VideoCapture(str(\"motion_vids/buddha_empty_close.MOV\"))\n",
    "frame = get_vid_frame(motion_clip,2.2)\n",
    "target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "\n",
    "\n",
    "optimizer.set_frame(target_frame_rgb)\n",
    "frame_dinos = optimizer.frame_pca_feats\n",
    "dino_feats = cam_outputs['dino']\n",
    "# to_pca = torch.cat([dino_feats.view(-1,dino_feats.shape[-1]),frame_to_pca.view(-1,frame_to_pca.shape[-1])],dim=0)\n",
    "to_pca = dino_feats.view(-1,dino_feats.shape[-1])\n",
    "_,_,rgb_pca = torch.pca_lowrank(to_pca, q=3, niter=30)\n",
    "dino_PCA = apply_pca_colormap(dino_feats,rgb_pca)\n",
    "dino_PCA[(dino_PCA==0).all(dim=-1)] = 1\n",
    "\n",
    "\n",
    "#add to viser\n",
    "R = vtf.SO3.from_matrix(camera.camera_to_worlds[0,:3,:3].cpu().numpy()) @ vtf.SO3.from_x_radians(np.pi)\n",
    "v.viser_server.add_camera_frustum(\n",
    "        name=f\"dino_rend_cam\",\n",
    "        fov=float(2 * np.arctan(camera.cx / camera.fx[0])),\n",
    "        scale=.3,\n",
    "        aspect=float(camera.cx[0] / camera.cy[0]),\n",
    "        wxyz=R.wxyz,\n",
    "        position=camera.camera_to_worlds[0,:3, 3] * 10,\n",
    "    )\n",
    "\n",
    "frame_map = apply_pca_colormap(frame_dinos, rgb_pca)\n",
    "_,ax = plt.subplots(1,5,figsize=(20,5),dpi=500)\n",
    "ax[0].imshow(target_frame_rgb.cpu().numpy())\n",
    "ax[1].imshow(dino_PCA.cpu().numpy())\n",
    "ax[2].imshow(frame_map.cpu().numpy())\n",
    "cam_outputs['depth'][cam_outputs['accumulation']<.8] = 0\n",
    "ax[3].imshow(cam_outputs['depth'].detach().cpu().numpy())\n",
    "ax[4].imshow(optimizer.frame_depth.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for visualizing a loaded mesh in viser \n",
    "# NOTE: this modifies in-place, so don't do it more than once\n",
    "# trial_mesh = optimizer.hand_frames[1][1]\n",
    "# trial_mesh.apply_transform(optimizer.get_registered_o2w().cpu().numpy())\n",
    "# v.viser_server.add_mesh_trimesh(\"trial_mesh\",trial_mesh,scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = optimizer.compute_two_hand_assignment()\n",
    "li,ri = assignments[0]\n",
    "\n",
    "lcentroid = optimizer.dig_model.means[optimizer.group_masks[li]].mean(0)\n",
    "v.viser_server.scene.add_label(\"lhand\",\"Left\", position=lcentroid.detach().cpu().numpy()*10)\n",
    "\n",
    "rcentroid = optimizer.dig_model.means[optimizer.group_masks[ri]].mean(0)\n",
    "v.viser_server.scene.add_label(\"rhand\",\"Right\", position=rcentroid.detach().cpu().numpy()*10)\n",
    "\"\"\"\n",
    "thumb: 745\n",
    "index: 349\n",
    "middle: 429\n",
    "ring finger: 554\n",
    "pinky: 692\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
