{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell loads the model from the config file and initializes the viewer\n",
    "'''\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nerfstudio.viewer.viewer import Viewer\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from nerfstudio.utils import writer\n",
    "import time\n",
    "from threading import Lock\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import resize\n",
    "from toad.zed import Zed\n",
    "import warp as wp\n",
    "from toad.optimization.rigid_group_optimizer import RigidGroupOptimizer\n",
    "from toad.optimization.atap_loss import ATAPLoss\n",
    "from toad.utils import *\n",
    "from toad.hand_registration import HandRegistration\n",
    "wp.init()\n",
    "\n",
    "# config = Path(\"outputs/nerfgun2/dig/2024-05-03_161203/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun3/dig/2024-05-03_170424/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun4/dig/2024-05-07_130351/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-10_132522/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-16_233028/config.yml\")#with ruilongs v2\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-09_123412/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-16_231213/config.yml\")#with ruilongs v2\n",
    "# config = Path(\"outputs/cal_bear/dig/2024-05-15_155531/config.yml\")#this one groups table with bear for some reason\n",
    "# config = Path(\"outputs/bww_faucet/dig/2024-05-12_215440/config.yml\")\n",
    "# config = Path(\"outputs/cmk_tpose2/dig/2024-05-14_142439/config.yml\")\n",
    "# config = Path(\"outputs/cal_bear/dig/2024-05-17_142920/config.yml\")#ruilong v2\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-17_145312/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger2/dig/2024-05-17_152545/config.yml\")\n",
    "# config = Path(\"outputs/glue_gun/dig/2024-05-17_161408/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-19_122050/config.yml\")# reuilong v2, 32-dim gauss\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-19_125443/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger2/dig/2024-05-19_132100/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-20_191616/config.yml\")#with antialias\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-20_192646/config.yml\")\n",
    "# config = Path(\"outputs/garfield_plushie/dig/2024-05-21_144709/config.yml\")\n",
    "# config=Path(\"outputs/buddha_balls/dig/2024-05-23_145359/config.yml\")\n",
    "# config=Path(\"outputs/buddha_balls_poly/dig/2024-05-23_153552/config.yml\")\n",
    "# config = Path(\"outputs/calbear/dig/2024-05-24_160735/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-29_133909/config.yml\")#justins machine\n",
    "config = Path(\"outputs/nerfgun3/dig/2024-05-29_185729/config.yml\")#justins machine\n",
    "OUTPUT_FOLDER = Path(\"renders/nerfgun\")\n",
    "\n",
    "assert OUTPUT_FOLDER.stem in str(config), \"Output folder name does not match config name\"\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True,parents=True)\n",
    "train_config,pipeline,_,_ = eval_setup(config)\n",
    "\n",
    "dino_loader = pipeline.datamanager.dino_dataloader\n",
    "train_config.logging.local_writer.enable = False\n",
    "# We need to set up the writer to track number of rays, otherwise the viewer will not calculate the resolution correctly\n",
    "writer.setup_local_writer(train_config.logging, max_iter=train_config.max_num_iterations)\n",
    "v = Viewer(ViewerConfig(default_composite_depth=False,num_rays_per_chunk=-1),config.parent,pipeline.datamanager.get_datapath(),pipeline,train_lock=Lock())\n",
    "try:\n",
    "    pipeline.load_state()\n",
    "except FileNotFoundError:\n",
    "    print(\"No state found, starting from scratch\")\n",
    "handreg = HandRegistration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell defines a simple pose optimizer for learning a rigid transform offset given a gaussian model, star pose, and starting view\n",
    "\"\"\"\n",
    "\n",
    "def get_vid_frame(cap,timestamp):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame number based on the timestamp and fps\n",
    "    frame_number = min(int(timestamp * fps),int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1))\n",
    "    \n",
    "    # Set the video position to the calculated frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    success, frame = cap.read()\n",
    "    # convert BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n",
    "\n",
    "MATCH_RESOLUTION = 500\n",
    "camera_input = 'iphone' # ['iphone','zed', 'iphone_vertical','zed_svo']\n",
    "video_path = Path(\"motion_vids/nerfgun_cock.MOV\")\n",
    "svo_path = Path(\"motion_vids/zed_box.svo2\")\n",
    "start_time = 0.3\n",
    "import viser.transforms as vtf\n",
    "cam_pose = None\n",
    "if cam_pose is None:\n",
    "    H = np.eye(4)\n",
    "    cam_pose = torch.from_numpy(H).float()[None,:3,:]#TODO ground truth cam to yumi\n",
    "if camera_input == 'iphone':\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fx = 1137.0,fy = 1137.0,cx = 1280.0/2,cy = 720/2,width=1280,height=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone_vertical':\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fy = 1137.0,fx = 1137.0,cy = 1280/2,cx = 720/2,height=1280,width=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input in ['zed','zed_svo']:\n",
    "    try:\n",
    "        zed.cam.close()\n",
    "        del zed\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        zed = Zed(recording_file=str(svo_path.absolute()) if camera_input == 'zed_svo' else None, start_time=start_time)\n",
    "    fps = 30    \n",
    "    left_rgb,_,_ = zed.get_frame()\n",
    "    K = zed.get_K()\n",
    "    init_cam = Cameras(camera_to_worlds=cam_pose,fx = K[0,0],fy = K[1,1],cx = K[0,2],cy = K[1,2],width=left_rgb.shape[1],height=left_rgb.shape[0])\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "outputs = pipeline.model.get_outputs_for_camera(init_cam)\n",
    "if pipeline.cluster_labels is not None:\n",
    "    labels = pipeline.cluster_labels.int().cuda()\n",
    "    group_masks = [(cid == labels).cuda() for cid in range(labels.max() + 1)]\n",
    "else:\n",
    "    labels = torch.zeros(pipeline.model.num_points).int().cuda()\n",
    "    group_masks = [torch.ones(pipeline.model.num_points).bool().cuda()]\n",
    "optimizer = RigidGroupOptimizer(pipeline.model,dino_loader,init_cam,group_masks, group_labels = labels, dataset_scale = pipeline.datamanager.train_dataset._dataparser_outputs.dataparser_scale, render_lock = v.train_lock)\n",
    "rgb_renders = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if camera_input in ['zed','zed_svo']:\n",
    "    left_rgb, right_rgb,depth = zed.get_frame()\n",
    "    target_frame_rgb = (left_rgb/255)\n",
    "    right_frame_rgb = (right_rgb/255)\n",
    "    optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "else:\n",
    "    assert video_path.exists()\n",
    "    motion_clip = cv2.VideoCapture(str(video_path.absolute()))\n",
    "    start=.75\n",
    "    end=1\n",
    "    fps = 30\n",
    "    frame = get_vid_frame(motion_clip,start)\n",
    "    target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "    optimizer.set_frame(target_frame_rgb)\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "xs,ys,outputs,renders = optimizer.initialize_obj_pose(render=True,niter=50,metric_depth=False)\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "rescale = max(target_frame_rgb.shape[0],target_frame_rgb.shape[1])/MATCH_RESOLUTION\n",
    "axs[1].scatter(xs.cpu().numpy()*rescale,ys.cpu().numpy()*rescale)\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())\n",
    "axs[0].imshow(optimizer.rgb_frame.cpu().numpy(),alpha=.3)\n",
    "if len(renders)>1:\n",
    "    renders = [r.detach().cpu().numpy()*255 for r in renders]\n",
    "    #save video as test_camopt.mp4\n",
    "    out_clip = mpy.ImageSequenceClip(renders, fps=30)  \n",
    "    out_clip.write_videofile(\"test_camopt.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import moviepy.editor as mpy\n",
    "import plotly.express as px\n",
    "import viser\n",
    "import trimesh\n",
    "\n",
    "def plotly_render(frame):\n",
    "    fig = px.imshow(frame)\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),showlegend=False,yaxis_visible=False, yaxis_showticklabels=False,xaxis_visible=False, xaxis_showticklabels=False\n",
    "    )\n",
    "    return fig\n",
    "fig = plotly_render(outputs['rgb'].detach().cpu().numpy())\n",
    "try:\n",
    "    frame_vis.remove()\n",
    "except:\n",
    "    pass\n",
    "frame_vis = pipeline.viewer_control.viser_server.add_gui_plotly(fig, 9/16)\n",
    "try:\n",
    "    animate_button.remove()\n",
    "    frame_slider.remove()\n",
    "    reset_button.remove()\n",
    "except:\n",
    "    pass\n",
    "def composite_vis_frame(target_frame_rgb,outputs):\n",
    "    target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "    # composite the outputs['rgb'] on top of target_vis frame\n",
    "    target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "    return target_vis_frame\n",
    "\n",
    "try:\n",
    "    render_button.remove()\n",
    "    filename_input.remove()\n",
    "    status_mkdown.remove()\n",
    "except:\n",
    "    pass\n",
    "filename_input = v.viser_server.add_gui_text(\"File Name\",\"render\")\n",
    "status_mkdown = v.viser_server.add_gui_markdown(\" \")\n",
    "render_button = v.viser_server.add_gui_button(\"Render Animation\",color='green',icon=viser.Icon.MOVIE)\n",
    "@render_button.on_click\n",
    "def render(_):\n",
    "    render_button.disabled = True\n",
    "    render_frames = []\n",
    "    camera = pipeline.viewer_control.get_camera(1080,1920,0)\n",
    "    for i in tqdm.tqdm(range(len(optimizer.keyframes))):\n",
    "        status_mkdown.content = f\"Rendering...{i/len(optimizer.keyframes):.01f}\"\n",
    "        pipeline.model.eval()\n",
    "        optimizer.apply_keyframe(i)\n",
    "        with torch.no_grad():\n",
    "            outputs = pipeline.model.get_outputs_for_camera(camera)\n",
    "        render_frames.append(outputs[\"rgb\"].detach().cpu().numpy()*255)\n",
    "    status_mkdown.content = \"Saving...\"\n",
    "    out_clip = mpy.ImageSequenceClip(render_frames, fps=fps)\n",
    "    fname = filename_input.value\n",
    "    (OUTPUT_FOLDER / 'posed_renders').mkdir(exist_ok=True)\n",
    "    render_folder = OUTPUT_FOLDER / 'posed_renders'\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}.mp4\", fps=fps,codec='libx264')\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}_mac.mp4\", fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "    v.viser_server.send_file_download(f\"{fname}_mac.mp4\",open(f\"{render_folder}/{fname}_mac.mp4\",'rb').read())\n",
    "    status_mkdown.content = \"Done!\"\n",
    "    render_button.disabled = False\n",
    "from typing import Tuple,Optional,List\n",
    "def get_hands(frame,optimizer) -> Tuple[Optional[List[trimesh.Trimesh]],Optional[List[trimesh.Trimesh]]]:\n",
    "    left_hand,right_hand = handreg.detect_hands(frame,optimizer.init_c2o.fx.item()*(max(frame.shape[0],frame.shape[1])/MATCH_RESOLUTION))\n",
    "    for i,hands in enumerate([left_hand,right_hand]):\n",
    "        if hands is None:continue\n",
    "        hands['trimeshes'] = []\n",
    "        #Compute hand shift to align with gaussians\n",
    "        handreg.align_hands(hands,outputs['depth'].detach()/optimizer.dataset_scale, 1/optimizer.frame_depth, outputs['accumulation'].detach()>.9,optimizer.init_c2o.fx.item())\n",
    "        # visualize result\n",
    "        for j in range(left_hand['verts'].shape[0]):\n",
    "            vertices = hands['verts'][j]\n",
    "            faces = hands['faces']\n",
    "            mesh = trimesh.Trimesh(vertices, faces)\n",
    "            cam_pose = torch.eye(4)\n",
    "            cam_pose[:3,:] = optimizer.init_c2o.camera_to_worlds\n",
    "            cam_pose[1:3,:] *= -1\n",
    "            mesh.apply_transform(cam_pose.cpu().numpy())\n",
    "\n",
    "            mesh.vertices = mesh.vertices*optimizer.dataset_scale\n",
    "            v.viser_server.add_mesh_trimesh(f\"hand{i}_{j}\",mesh,scale=10)\n",
    "            hands['trimeshes'].append(mesh)\n",
    "    return [] if left_hand is None else left_hand['trimeshes'],[] if right_hand is None else right_hand['trimeshes']\n",
    "\n",
    "lh,rh = get_hands(target_frame_rgb.cpu().numpy(),optimizer)\n",
    "optimizer.register_keyframe(lhands = lh, rhands = rh)\n",
    "\n",
    "if camera_input in ['zed','zed_svo']:\n",
    "    while True:\n",
    "        # If input camera is the zed, just loop it indefinitely until no more frames\n",
    "        left_rgb, _, depth = zed.get_frame()\n",
    "        if left_rgb is None:\n",
    "            break\n",
    "        target_frame_rgb = left_rgb/255\n",
    "        optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "        outputs = optimizer.step(50, metric_depth=True)\n",
    "        v._trigger_rerender()\n",
    "        lhands,rhands = get_hands(target_frame_rgb,optimizer)\n",
    "        optimizer.register_keyframe(lhands = lhands, rhands = rhands)\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "elif camera_input in ['iphone','iphone_vertical','train_cam']:\n",
    "    # Otherwise procces the video\n",
    "    for t in tqdm.tqdm(np.linspace(start,end,int((end-start)*fps))):\n",
    "        frame = get_vid_frame(motion_clip,t)\n",
    "        target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "        optimizer.set_frame(target_frame_rgb)\n",
    "        outputs = optimizer.step(100, metric_depth=False)\n",
    "        lhands,rhands = get_hands(frame,optimizer)\n",
    "        optimizer.register_keyframe(lhands = lhands, rhands = rhands)\n",
    "        v._trigger_rerender()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "#save as an mp4\n",
    "out_clip = mpy.ImageSequenceClip(rgb_renders, fps=fps)  \n",
    "\n",
    "#save rendering video\n",
    "fname = str(OUTPUT_FOLDER / \"optimizer_out.mp4\")\n",
    "out_clip.write_videofile(fname, fps=fps,codec='libx264')\n",
    "out_clip.write_videofile(fname.replace('.mp4','_mac.mp4'),fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "\n",
    "#save part trajectories\n",
    "optimizer.save_trajectory(OUTPUT_FOLDER / \"keyframes.pt\")\n",
    "print(\"Saved keyframes to\",OUTPUT_FOLDER / \"keyframes.pt\")\n",
    "\n",
    "# Populate some viewer elements to visualize the animation\n",
    "animate_button = v.viser_server.add_gui_button(\"Play Animation\")\n",
    "frame_slider = v.viser_server.add_gui_slider(\"Frame\",0,len(optimizer.keyframes)-1,1,0)\n",
    "reset_button = v.viser_server.add_gui_button(\"Reset Transforms\")\n",
    "\n",
    "@animate_button.on_click\n",
    "def play_animation(_):\n",
    "    for i in range(len(optimizer.keyframes)):\n",
    "        optimizer.apply_keyframe(i)\n",
    "        v._trigger_rerender()\n",
    "        time.sleep(1/fps)\n",
    "@frame_slider.on_update\n",
    "def apply_keyframe(_):\n",
    "    optimizer.apply_keyframe(frame_slider.value)\n",
    "    v._trigger_rerender()\n",
    "@reset_button.on_click\n",
    "def reset_transforms(_):\n",
    "    optimizer.reset_transforms()\n",
    "    v._trigger_rerender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for visualizing a loaded mesh in viser \n",
    "# NOTE: this modifies in-place, so don't do it more than once\n",
    "# trial_mesh = optimizer.hand_frames[1][1]\n",
    "# trial_mesh.apply_transform(optimizer.get_registered_o2w().cpu().numpy())\n",
    "# v.viser_server.add_mesh_trimesh(\"trial_mesh\",trial_mesh,scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
