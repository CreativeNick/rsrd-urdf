{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell loads the model from the config file and initializes the viewer\n",
    "'''\n",
    "# %matplotlib widget\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from nerfstudio.viewer.viewer import Viewer\n",
    "from nerfstudio.configs.base_config import ViewerConfig\n",
    "import cv2\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from nerfstudio.utils import writer\n",
    "import time\n",
    "from threading import Lock\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import resize\n",
    "from lerf.zed import Zed\n",
    "import warp as wp\n",
    "from toad.optimization.rigid_group_optimizer import RigidGroupOptimizer\n",
    "from toad.optimization.atap_loss import ATAPLoss\n",
    "from toad.utils import *\n",
    "wp.init()\n",
    "\n",
    "# config = Path(\"outputs/nerfgun2/dig/2024-05-03_161203/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun3/dig/2024-05-03_170424/config.yml\")\n",
    "# config = Path(\"outputs/nerfgun4/dig/2024-05-07_130351/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-10_132522/config.yml\")\n",
    "# config = Path(\"outputs/painter_sculpture/dig/2024-05-16_233028/config.yml\")#with ruilongs v2\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-09_123412/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-16_231213/config.yml\")#with ruilongs v2\n",
    "# config = Path(\"outputs/cal_bear/dig/2024-05-15_155531/config.yml\")#this one groups table with bear for some reason\n",
    "# config = Path(\"outputs/bww_faucet/dig/2024-05-12_215440/config.yml\")\n",
    "# config = Path(\"outputs/cmk_tpose2/dig/2024-05-14_142439/config.yml\")\n",
    "# config = Path(\"outputs/cal_bear/dig/2024-05-17_142920/config.yml\")#ruilong v2\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-17_145312/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger2/dig/2024-05-17_152545/config.yml\")\n",
    "# config = Path(\"outputs/glue_gun/dig/2024-05-17_161408/config.yml\")\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-19_122050/config.yml\")# reuilong v2, 32-dim gauss\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-19_125443/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger2/dig/2024-05-19_132100/config.yml\")\n",
    "# config = Path(\"outputs/mac_charger/dig/2024-05-20_191616/config.yml\")#with antialias\n",
    "# config = Path(\"outputs/buddha_balls_poly/dig/2024-05-20_192646/config.yml\")\n",
    "# config = Path(\"outputs/garfield_plushie/dig/2024-05-21_144709/config.yml\")\n",
    "config=Path(\"outputs/boops_poly/dig/2024-05-22_000924/config.yml\")\n",
    "OUTPUT_FOLDER = Path(\"renders/boops_poly\")\n",
    "\n",
    "assert OUTPUT_FOLDER.stem in str(config), \"Output folder name does not match config name\"\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True,parents=True)\n",
    "train_config,pipeline,_,_ = eval_setup(config)\n",
    "dino_loader = pipeline.datamanager.dino_dataloader\n",
    "train_config.logging.local_writer.enable = False\n",
    "# We need to set up the writer to track number of rays, otherwise the viewer will not calculate the resolution correctly\n",
    "writer.setup_local_writer(train_config.logging, max_iter=train_config.max_num_iterations)\n",
    "v = Viewer(ViewerConfig(default_composite_depth=False,num_rays_per_chunk=-1),config.parent,pipeline.datamanager.get_datapath(),pipeline,train_lock=Lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'camera_to_worlds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     init_cam\u001b[38;5;241m.\u001b[39mrescale_output_resolution(MATCH_RESOLUTION\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmax\u001b[39m(init_cam\u001b[38;5;241m.\u001b[39mwidth,init_cam\u001b[38;5;241m.\u001b[39mheight))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m camera_input \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miphone_vertical\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     init_cam \u001b[38;5;241m=\u001b[39m Cameras(camera_to_worlds\u001b[38;5;241m=\u001b[39m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer_control\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_camera\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcamera_to_worlds\u001b[49m,fy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1137.0\u001b[39m,fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1137.0\u001b[39m,cy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1280\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m,cx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m720\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m,height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m,width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m720\u001b[39m)\n\u001b[1;32m     37\u001b[0m     init_cam\u001b[38;5;241m.\u001b[39mrescale_output_resolution(MATCH_RESOLUTION\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmax\u001b[39m(init_cam\u001b[38;5;241m.\u001b[39mwidth,init_cam\u001b[38;5;241m.\u001b[39mheight))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m camera_input \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzed\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzed_svo\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'camera_to_worlds'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell defines a simple pose optimizer for learning a rigid transform offset given a gaussian model, star pose, and starting view\n",
    "\"\"\"\n",
    "\n",
    "def get_vid_frame(cap,timestamp):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame number based on the timestamp and fps\n",
    "    frame_number = min(int(timestamp * fps),int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1))\n",
    "    \n",
    "    # Set the video position to the calculated frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    success, frame = cap.read()\n",
    "    # convert BGR to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return frame\n",
    "\n",
    "MATCH_RESOLUTION = 500\n",
    "camera_input = 'iphone_vertical' # ['train_cam', 'iphone','zed', 'iphone_vertical','zed_svo']\n",
    "video_path = Path(\"motion_vids/boops_lift.MOV\")\n",
    "svo_path = Path(\"motion_vids/buddha_remove_good.svo2\")\n",
    "start_time = 0.3\n",
    "\n",
    "cam_pose = pipeline.viewer_control.get_camera(200,None,0)\n",
    "if cam_pose is None:\n",
    "    cam_pose = torch.eye(4).float().cuda()\n",
    "if camera_input == 'train_cam':\n",
    "    init_cam,data = pipeline.datamanager.next_train(0)\n",
    "    view_cam_pose = pipeline.viewer_control.get_camera(200,None,0)\n",
    "    init_cam.camera_to_worlds = view_cam_pose.camera_to_worlds\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone':\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fx = 1137.0,fy = 1137.0,cx = 1280.0/2,cy = 720/2,width=1280,height=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input == 'iphone_vertical':\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fy = 1137.0,fx = 1137.0,cy = 1280/2,cx = 720/2,height=1280,width=720)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "elif camera_input in ['zed','zed_svo']:\n",
    "    try:\n",
    "        zed.cam.close()\n",
    "        del zed\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        zed = Zed(recording_file=str(svo_path.absolute()) if camera_input == 'zed_svo' else None, start_time=start_time)\n",
    "    fps = 30\n",
    "    left_rgb,_,_ = zed.get_frame()\n",
    "    K = zed.get_K()\n",
    "    init_cam = Cameras(camera_to_worlds=pipeline.viewer_control.get_camera(200,None,0).camera_to_worlds,fx = K[0,0],fy = K[1,1],cx = K[0,2],cy = K[1,2],width=1920,height=1080)\n",
    "    init_cam.rescale_output_resolution(MATCH_RESOLUTION/max(init_cam.width,init_cam.height))\n",
    "outputs = pipeline.model.get_outputs_for_camera(init_cam)\n",
    "if pipeline.cluster_labels is not None:\n",
    "    labels = pipeline.cluster_labels.int().cuda()\n",
    "    group_masks = [(cid == labels).cuda() for cid in range(labels.max() + 1)]\n",
    "else:\n",
    "    labels = torch.zeros(pipeline.model.num_points).int().cuda()\n",
    "    group_masks = [torch.ones(pipeline.model.num_points).bool().cuda()]\n",
    "optimizer = RigidGroupOptimizer(pipeline.model,dino_loader,init_cam,group_masks, group_labels = labels, render_lock = v.train_lock)\n",
    "rgb_renders = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if camera_input in ['zed','zed_svo']:\n",
    "    left_rgb, right_rgb,depth = zed.get_frame()\n",
    "    target_frame_rgb = (left_rgb/255)\n",
    "    right_frame_rgb = (right_rgb/255)\n",
    "    optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "else:\n",
    "    assert video_path.exists()\n",
    "    motion_clip = cv2.VideoCapture(str(video_path.absolute()))\n",
    "    start=1\n",
    "    end=9\n",
    "    fps = 30\n",
    "    frame = get_vid_frame(motion_clip,start)\n",
    "    target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "    optimizer.set_frame(target_frame_rgb)\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "xs,ys,outputs,best_pose,renders,best_pix = optimizer.initialize_obj_pose()\n",
    "_,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(outputs[\"rgb\"].detach().cpu().numpy())\n",
    "rescale = max(target_frame_rgb.shape[0],target_frame_rgb.shape[1])/MATCH_RESOLUTION\n",
    "best_pix = best_pix*rescale\n",
    "axs[1].scatter(xs.cpu().numpy()*rescale,ys.cpu().numpy()*rescale)\n",
    "axs[1].imshow(target_frame_rgb.cpu().numpy())\n",
    "axs[0].imshow(optimizer.rgb_frame.cpu().numpy(),alpha=.3)\n",
    "renders = [r.detach().cpu().numpy()*255 for r in renders]\n",
    "#save video as test_camopt.mp4\n",
    "out_clip = mpy.ImageSequenceClip(renders, fps=30)  \n",
    "out_clip.write_videofile(\"test_camopt.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "import tqdm\n",
    "import moviepy.editor as mpy\n",
    "import plotly.express as px\n",
    "def plotly_render(frame):\n",
    "    fig = px.imshow(frame)\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),showlegend=False,yaxis_visible=False, yaxis_showticklabels=False,xaxis_visible=False, xaxis_showticklabels=False\n",
    "    )\n",
    "    return fig\n",
    "fig = plotly_render(outputs['rgb'].detach().cpu().numpy())\n",
    "try:\n",
    "    frame_vis.remove()\n",
    "except:\n",
    "    pass\n",
    "frame_vis = pipeline.viewer_control.viser_server.add_gui_plotly(fig, 9/16)\n",
    "try:\n",
    "    animate_button.remove()\n",
    "    frame_slider.remove()\n",
    "    reset_button.remove()\n",
    "except:\n",
    "    pass\n",
    "def composite_vis_frame(target_frame_rgb,outputs):\n",
    "    target_vis_frame = resize(target_frame_rgb.permute(2,0,1),(outputs[\"rgb\"].shape[0],outputs[\"rgb\"].shape[1])).permute(1,2,0)\n",
    "    # composite the outputs['rgb'] on top of target_vis frame\n",
    "    target_vis_frame = target_vis_frame*0.5 + outputs[\"rgb\"]*0.5\n",
    "    return target_vis_frame\n",
    "\n",
    "try:\n",
    "    render_button.remove()\n",
    "    filename_input.remove()\n",
    "    status_mkdown.remove()\n",
    "except:\n",
    "    pass\n",
    "import viser\n",
    "filename_input = v.viser_server.add_gui_text(\"File Name\",\"render\")\n",
    "status_mkdown = v.viser_server.add_gui_markdown(\" \")\n",
    "render_button = v.viser_server.add_gui_button(\"Render Animation\",color='green',icon=viser.Icon.MOVIE)\n",
    "@render_button.on_click\n",
    "def render(_):\n",
    "    render_button.disabled = True\n",
    "    render_frames = []\n",
    "    camera = pipeline.viewer_control.get_camera(1080,1920,0)\n",
    "    for i in tqdm.tqdm(range(len(optimizer.keyframes))):\n",
    "        status_mkdown.content = f\"Rendering...{i/len(optimizer.keyframes):.01f}\"\n",
    "        pipeline.model.eval()\n",
    "        optimizer.apply_keyframe(i)\n",
    "        with torch.no_grad():\n",
    "            outputs = pipeline.model.get_outputs_for_camera(camera)\n",
    "        render_frames.append(outputs[\"rgb\"].detach().cpu().numpy()*255)\n",
    "    status_mkdown.content = \"Saving...\"\n",
    "    out_clip = mpy.ImageSequenceClip(render_frames, fps=fps)\n",
    "    fname = filename_input.value\n",
    "    (OUTPUT_FOLDER / 'posed_renders').mkdir(exist_ok=True)\n",
    "    render_folder = OUTPUT_FOLDER / 'posed_renders'\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}.mp4\", fps=fps,codec='libx264')\n",
    "    out_clip.write_videofile(f\"{render_folder}/{fname}_mac.mp4\", fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "    v.viser_server.send_file_download(f\"{fname}_mac.mp4\",open(f\"{render_folder}/{fname}_mac.mp4\",'rb').read())\n",
    "    status_mkdown.content = \"Done!\"\n",
    "    render_button.disabled = False\n",
    "\n",
    "\n",
    "if camera_input in ['zed','zed_svo']:\n",
    "    if len(rgb_renders)==0:\n",
    "        for i in tqdm.tqdm(range(10)):\n",
    "            target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "            vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "            fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "            frame_vis.figure = fig\n",
    "            rgb_renders.append(vis_frame*255)\n",
    "            outputs = optimizer.step(50, use_depth=i>7, metric_depth=True)\n",
    "    while True:\n",
    "        # If input camera is the zed, just loop it indefinitely until no more frames\n",
    "        left_rgb, _, depth = zed.get_frame()\n",
    "        if left_rgb is None:\n",
    "            break\n",
    "        target_frame_rgb = left_rgb/255\n",
    "        optimizer.set_frame(target_frame_rgb,depth=depth)\n",
    "        outputs = optimizer.step(50, metric_depth=True)\n",
    "        v._trigger_rerender()\n",
    "        optimizer.register_keyframe()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "elif camera_input in ['iphone','iphone_vertical','train_cam']:\n",
    "    # Otherwise procces the video\n",
    "    if len(rgb_renders)==0:\n",
    "        for i in tqdm.tqdm(range(10)):\n",
    "            target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "            vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "            fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "            frame_vis.figure = fig\n",
    "            rgb_renders.append(vis_frame*255)\n",
    "            outputs = optimizer.step(30, use_depth=i>7, metric_depth=False)\n",
    "\n",
    "    for t in tqdm.tqdm(np.linspace(start,end,int((end-start)*fps))):\n",
    "        frame = get_vid_frame(motion_clip,t)\n",
    "        target_frame_rgb = ToTensor()(Image.fromarray(frame)).permute(1,2,0).cuda()\n",
    "        optimizer.set_frame(target_frame_rgb)\n",
    "        outputs = optimizer.step(50, metric_depth=False)\n",
    "        optimizer.register_keyframe()\n",
    "        v._trigger_rerender()\n",
    "        target_vis_frame = composite_vis_frame(target_frame_rgb,outputs)\n",
    "        vis_frame = torch.concatenate([outputs[\"rgb\"],target_vis_frame],dim=1).detach().cpu().numpy()\n",
    "        fig = plotly_render(target_vis_frame.detach().cpu().numpy())\n",
    "        frame_vis.figure = fig\n",
    "        rgb_renders.append(vis_frame*255)\n",
    "#save as an mp4\n",
    "out_clip = mpy.ImageSequenceClip(rgb_renders, fps=fps)  \n",
    "\n",
    "fname = str(OUTPUT_FOLDER / \"optimizer_out_antialias_withdepth_highatap.mp4\")\n",
    "\n",
    "out_clip.write_videofile(fname, fps=fps,codec='libx264')\n",
    "out_clip.write_videofile(fname.replace('.mp4','_mac.mp4'),fps=fps,codec='mpeg4',bitrate='5000k')\n",
    "\n",
    "# Populate some viewer elements to visualize the animation\n",
    "animate_button = v.viser_server.add_gui_button(\"Play Animation\")\n",
    "frame_slider = v.viser_server.add_gui_slider(\"Frame\",0,len(optimizer.keyframes)-1,1,0)\n",
    "reset_button = v.viser_server.add_gui_button(\"Reset Transforms\")\n",
    "\n",
    "@animate_button.on_click\n",
    "def play_animation(_):\n",
    "    for i in range(len(optimizer.keyframes)):\n",
    "        optimizer.apply_keyframe(i)\n",
    "        v._trigger_rerender()\n",
    "        time.sleep(1/fps)\n",
    "@frame_slider.on_update\n",
    "def apply_keyframe(_):\n",
    "    optimizer.apply_keyframe(frame_slider.value)\n",
    "    v._trigger_rerender()\n",
    "@reset_button.on_click\n",
    "def reset_transforms(_):\n",
    "    optimizer.reset_transforms()\n",
    "    v._trigger_rerender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "please",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
